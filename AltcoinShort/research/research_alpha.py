#!/usr/bin/env python3
"""
research_alpha.py  â€“  AltcoinShort èµ”ç‡/æ³¢åŠ¨ç‡/è¡Œä¸º Alpha ç»¼åˆç ”ç©¶
===================================================================

ç‹¬ç«‹è„šæœ¬ï¼Œä¸ä¾èµ– QuantConnect å¼•æ“ã€‚
ç›´æ¥è¯»å–æœ¬åœ° minute æ•°æ®ï¼Œè®¡ç®— 7 å¤§ alpha å› å­ï¼Œ
æ¨¡æ‹Ÿåšç©ºä¿¡å·å¹¶ç”Ÿæˆ PnL / å› å­åˆ†æã€‚

å› å­åˆ—è¡¨:
  A ç±» (æ³¢åŠ¨ç‡/èµ”ç‡):
    1. Downside Asymmetry
    2. Return Skewness
    3. Volatility Expansion
    4. Adverse Spike Filter
  B ç±» (è¡Œä¸ºé‡‘è):
    5. TS-CO   (æŒç»­è¿‡åº¦ååº”)
    6. PGR/PLR (å¤„ç½®æ•ˆåº” VWAP ä»£ç†)
    7. CSAD    (æ¨ªæˆªé¢ç¾Šç¾¤æ•ˆåº”)

ç”¨æ³•:
  /Users/chenzhao/Documents/lean_workspace/venv/bin/python research_alpha.py

å›æµ‹åŒºé—´: 2025â€‘11â€‘01 â€“ 2026â€‘02â€‘01
"""

from __future__ import annotations

import os
import zipfile
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from collections import deque

import numpy as np
import pandas as pd
from scipy import stats as sp_stats

warnings.filterwarnings("ignore")

# ====================================================================
# é…ç½®
# ====================================================================
DATA_DIR = Path(
    "/Users/chenzhao/Documents/lean_workspace/data/cryptofuture/binance/minute"
)
START_DATE = datetime(2025, 11, 1)
END_DATE = datetime(2026, 2, 1)

# A ç±»å› å­å‚æ•°
VOL_LOOKBACK_HOURS = 168
VOL_REFRESH_HOURS = 6
SKEW_THRESHOLD = -0.3
VOL_EXPANSION_CAP = 3.0
ADVERSE_SPIKE_MULT = 2.0
MAX_ADVERSE_RATIO = 2.0
WEIGHT_POWER = 1.5

# B ç±»å› å­å‚æ•°
CO_LOOKBACK = 24  # CO å›æœ›å°æ—¶
CO_EXTREME_PCT = 0.98  # CO æç«¯åˆ†ä½ (Tuned)
CO_DECAY = 0.95  # CO æŒ‡æ•°è¡°å‡
VWAP_LOOKBACK = 168  # PGR/PLR VWAP å›æœ›
DISPOSITION_HIGH = 0.03  # é«˜æµ®ç›ˆé˜ˆå€¼ 3%
DISPOSITION_LOW = -0.03  # é«˜æµ®äºé˜ˆå€¼ -3%
CSAD_LOOKBACK = 48  # CSAD å›å½’çª—å£ (Tuned)

# äº¤æ˜“å‚æ•°
MAX_POSITIONS = 5
HOLD_HOURS = 24
INITIAL_CAPITAL = 100_000
LEVERAGE = 2
TRAILING_STOP_PCT = 0.05
HARD_STOP_PCT = 0.10

# æ’é™¤æ¸…å•
EXCLUSION = {
    "ONDO",
    "PENDLE",
    "OM",
    "CFG",
    "POLYX",
    "MPL",
    "CPOOL",
    "MAPLE",
    "RIO",
    "PROPS",
    "RSR",
    "MKR",
    "COMP",
    "FET",
    "RENDER",
    "TAO",
    "WLD",
    "ARKM",
    "OCEAN",
    "AGIX",
    "ASI",
    "AKT",
    "RNDR",
    "GLM",
    "NMR",
    "CTXC",
    "PHB",
    "MDT",
    "VIRTUAL",
    "AI16Z",
    "GRIFFAIN",
    "PRIME",
    "AIOZ",
    "NFP",
    "ALI",
    "RSS3",
    "MASA",
    "IO",
    "PAAL",
    "SLEEPLESS",
    "0X0",
    "USDC",
    "USDT",
    "DAI",
    "TUSD",
    "USDP",
    "GUSD",
    "FRAX",
    "LUSD",
    "SUSD",
    "FDUSD",
    "PYUSD",
    "EURC",
    "PAXG",
    "XAUT",
    "BTC",
    "ETH",
}


# ====================================================================
# æ•°æ®åŠ è½½
# ====================================================================
def _load_minute_zip(zip_path: Path) -> Optional[pd.DataFrame]:
    try:
        with zipfile.ZipFile(zip_path) as zf:
            name = zf.namelist()[0]
            df = pd.read_csv(
                zf.open(name),
                header=None,
                names=["ms", "open", "high", "low", "close", "volume"],
            )
        return df
    except Exception:
        return None


def load_hourly_data(
    ticker_dir: Path, start: datetime, end: datetime
) -> Optional[pd.DataFrame]:
    frames: List[pd.DataFrame] = []
    d = start
    while d < end:
        fname = f"{d.strftime('%Y%m%d')}_trade.zip"
        zp = ticker_dir / fname
        if zp.exists():
            df = _load_minute_zip(zp)
            if df is not None and len(df) == 1440:
                base_ts = pd.Timestamp(d)
                df["datetime"] = base_ts + pd.to_timedelta(df["ms"], unit="ms")
                frames.append(df)
        d += timedelta(days=1)

    if not frames:
        return None

    full = pd.concat(frames, ignore_index=True)
    full.set_index("datetime", inplace=True)
    hourly = (
        full.resample("1h")
        .agg(
            {
                "open": "first",
                "high": "max",
                "low": "min",
                "close": "last",
                "volume": "sum",
            }
        )
        .dropna()
    )
    return hourly


# ====================================================================
# Alpha å› å­è®¡ç®—  (per-symbol)
# ====================================================================
def compute_factors(
    hourly: pd.DataFrame,
    co_history: deque,
) -> List[Dict]:
    """
    æ»‘åŠ¨çª—å£è®¡ç®—å…¨éƒ¨ 7 å› å­ã€‚
    è¿”å› [{datetime, å› å­å€¼..., weight, close}, ...]
    """
    closes = hourly["close"].values
    volumes = hourly["volume"].values
    returns = np.diff(closes) / closes[:-1]

    results = []

    for i in range(VOL_LOOKBACK_HOURS, len(returns), VOL_REFRESH_HOURS):
        window = returns[i - VOL_LOOKBACK_HOURS : i]
        if len(window) < 48:
            continue

        down_ret = window[window < 0]
        up_ret = window[window > 0]
        if len(down_ret) < 10 or len(up_ret) < 10:
            continue

        down_dev = np.std(down_ret)
        up_dev = np.std(up_ret)
        down_vol = down_dev * np.sqrt(24) * 100
        up_vol = up_dev * np.sqrt(24) * 100

        if (down_dev + up_dev) == 0:
            continue

        # â”€â”€ A1: Asymmetry â”€â”€
        asymmetry = (down_dev - up_dev) / (down_dev + up_dev)

        # â”€â”€ A2: Skewness â”€â”€
        skewness = float(sp_stats.skew(window))

        # â”€â”€ A3: Vol Expansion â”€â”€
        recent = window[-24:]
        vol_recent = np.std(recent) * np.sqrt(24) * 100
        vol_full = np.std(window) * np.sqrt(24) * 100
        vol_expansion = min(
            vol_recent / vol_full if vol_full > 0 else 1.0, VOL_EXPANSION_CAP
        )

        # â”€â”€ A4: Adverse Spike â”€â”€
        recent_up = recent[recent > 0]
        adverse_spike = False
        if len(recent_up) > 0:
            if np.max(recent_up) > ADVERSE_SPIKE_MULT * np.mean(np.abs(up_ret)):
                adverse_spike = True

        # Adverse vol ratio filter
        if up_vol > 0 and down_vol > 0 and (up_vol / down_vol) > MAX_ADVERSE_RATIO:
            continue
        if adverse_spike:
            continue

        # â”€â”€ B1: TS-CO (æŒç»­è¿‡åº¦ååº”) â”€â”€
        co_window = returns[max(0, i - CO_LOOKBACK) : i]
        decay_weights = np.array(
            [CO_DECAY**k for k in range(len(co_window) - 1, -1, -1)]
        )
        co_score = float(np.sum(np.sign(co_window) * decay_weights))
        co_history.append(co_score)

        co_extreme = False
        if len(co_history) > 100:
            pct_val = np.percentile(list(co_history), CO_EXTREME_PCT * 100)
            if abs(co_score) > abs(pct_val):
                co_extreme = True

        # â”€â”€ B2: PGR/PLR Proxy (VWAP ä¹–ç¦»ç‡) â”€â”€
        vwap_start = max(0, i + 1 - VWAP_LOOKBACK)
        vwap_end = i + 1
        vwap_closes = closes[vwap_start:vwap_end]
        vwap_vols = volumes[vwap_start:vwap_end]
        total_vv = np.sum(vwap_vols)
        if total_vv > 0:
            vwap = np.sum(vwap_closes * vwap_vols) / total_vv
        else:
            vwap = np.mean(vwap_closes)
        current_close = closes[i] if i < len(closes) else closes[-1]
        disposition = (current_close - vwap) / vwap if vwap > 0 else 0.0

        # â”€â”€ ç»¼åˆæƒé‡ â”€â”€
        base_score = max(asymmetry, 0.0)

        skew_bonus = 1.0
        if skewness < SKEW_THRESHOLD:
            skew_bonus = 1.0 + min(abs(skewness) * 0.5, 1.0)

        vol_mult = max(vol_expansion, 1.0)

        # CO modifier (Tuned)
        co_mult = 1.0
        if co_extreme:
            if co_score < 0:
                co_mult = 0.5  # è¿ç»­æš´è·Œåˆ°æç«¯ â†’ é€‚åº¦é™æƒ
            else:
                co_mult = 1.5  # è¿ç»­æš´æ¶¨åˆ°æç«¯ â†’ å´©ç›˜ä¿¡å· â†’ åšç©ºåŠ åˆ†
        else:
            if co_score < -2:
                co_mult = 1.2  # æŒç»­ä¸‹è·Œä¸­ â†’ è¶‹åŠ¿å»¶ç»­ â†’ å°å¹…åŠ åˆ†

        # Disposition modifier (Tuned)
        disp_mult = 1.0
        if disposition > DISPOSITION_HIGH:
            disp_mult = 1.0 + min(disposition * 5, 1.0)  # é«˜æµ®ç›ˆ â†’ å–å‹
        elif disposition < DISPOSITION_LOW:
            disp_mult = max(1.0 + disposition * 2, 0.3)  # é«˜æµ®äº â†’ æ”¯æ’‘

        weight = (
            (base_score**WEIGHT_POWER) * skew_bonus * vol_mult * co_mult * disp_mult
        )

        dt_idx = i + 1
        if dt_idx < len(hourly):
            results.append(
                {
                    "datetime": hourly.index[dt_idx],
                    "asymmetry": round(asymmetry, 4),
                    "skewness": round(skewness, 3),
                    "vol_expansion": round(vol_expansion, 3),
                    "co_score": round(co_score, 3),
                    "co_extreme": co_extreme,
                    "disposition": round(disposition, 4),
                    "weight": round(weight, 4),
                    "close": closes[dt_idx] if dt_idx < len(closes) else closes[-1],
                    "down_vol": round(down_vol, 2),
                    "up_vol": round(up_vol, 2),
                }
            )

    return results


# ====================================================================
# CSAD æ¨ªæˆªé¢è®¡ç®—
# ====================================================================
def compute_csad_series(
    all_hourly: Dict[str, pd.DataFrame],
    start: datetime,
    end: datetime,
) -> pd.DataFrame:
    """
    è®¡ç®—æ¨ªæˆªé¢ CSAD åŠå›å½’ç³»æ•° Î³2 çš„æ»šåŠ¨å€¼ã€‚
    è¿”å› DataFrame: [datetime, csad, r_market, gamma2, herding]
    """
    # æ„å»ºç»Ÿä¸€æ—¶é—´ç´¢å¼•çš„ return panel
    return_dfs = {}
    for ticker, h in all_hourly.items():
        c = h["close"]
        r = c.pct_change().dropna()
        r = r[(r.index >= pd.Timestamp(start)) & (r.index < pd.Timestamp(end))]
        if len(r) > 100:
            return_dfs[ticker] = r

    if len(return_dfs) < 5:
        return pd.DataFrame()

    panel = pd.DataFrame(return_dfs)
    panel.dropna(axis=0, how="all", inplace=True)
    # è‡³å°‘ 5 ä¸ª ticker æœ‰æ•°æ®çš„è¡Œ
    panel = panel.dropna(thresh=5)

    if len(panel) < CSAD_LOOKBACK * 2:
        return pd.DataFrame()

    r_market = panel.mean(axis=1)
    csad = panel.sub(r_market, axis=0).abs().mean(axis=1)

    # æ»šåŠ¨å›å½’
    results = []
    for end_idx in range(CSAD_LOOKBACK, len(panel)):
        start_idx = end_idx - CSAD_LOOKBACK
        y = csad.iloc[start_idx:end_idx].values
        rm = r_market.iloc[start_idx:end_idx].values

        X = np.column_stack([np.ones(CSAD_LOOKBACK), np.abs(rm), rm**2])
        try:
            beta = np.linalg.lstsq(X, y, rcond=None)[0]
            gamma2 = float(beta[2])
            resid = y - X @ beta
            s2 = np.sum(resid**2) / (CSAD_LOOKBACK - 3)
            var_b = s2 * np.linalg.inv(X.T @ X)
            t_stat = gamma2 / np.sqrt(var_b[2, 2]) if var_b[2, 2] > 0 else 0.0
            herding = gamma2 < 0 and t_stat < -2.0
        except Exception:
            gamma2 = 0.0
            herding = False

        results.append(
            {
                "datetime": panel.index[end_idx],
                "csad": float(csad.iloc[end_idx]),
                "r_market": float(r_market.iloc[end_idx]),
                "gamma2": gamma2,
                "herding": herding,
            }
        )

    return pd.DataFrame(results)


# ====================================================================
# å›æµ‹å¼•æ“ (Optimized Exit)
# ====================================================================
class Position:
    __slots__ = (
        "ticker",
        "entry_price",
        "entry_time",
        "initial_size",
        "current_size",
        "best_price",
        "pnl",
        "closed",
        "close_reason",
        "partial_taken",
        "max_pnl_pct",
    )

    def __init__(self, ticker, entry_price, entry_time, size):
        self.ticker = ticker
        self.entry_price = entry_price
        self.entry_time = entry_time
        self.initial_size = size
        self.current_size = size
        self.best_price = entry_price
        self.pnl = 0.0
        self.closed = False
        self.close_reason = ""
        self.partial_taken = False
        self.max_pnl_pct = 0.0

    def update(self, current_price, current_time) -> bool:
        self.best_price = min(self.best_price, current_price)

        # Current Unrealized PnL % (Short)
        current_pnl_pct = (self.entry_price - current_price) / self.entry_price
        self.max_pnl_pct = max(self.max_pnl_pct, current_pnl_pct)

        elapsed_hours = (current_time - self.entry_time).total_seconds() / 3600

        # 1. Flash Crash TP: Close 50% if >8% profit in <4h
        if not self.partial_taken and elapsed_hours <= 4 and current_pnl_pct > 0.08:
            realized = self.current_size * 0.5 * current_pnl_pct
            self.pnl += realized
            self.current_size *= 0.5
            self.partial_taken = True
            # Note: We don't return True here, trade continues with half size

        # 2. Hard Stop (10%)
        # Check against entry price
        loss_pct = (current_price - self.entry_price) / self.entry_price
        if loss_pct > HARD_STOP_PCT:
            # Close remaining
            self.pnl += -loss_pct * self.current_size
            self.closed = True
            self.close_reason = "hard_stop"
            return True

        # 3. Dynamic Trailing Stop
        # Default 5%. If profit > 10%, tighten to 2%.
        trailing_limit = 0.02 if self.max_pnl_pct > 0.10 else 0.05

        # Calculate bounce from best price
        bounce = (current_price - self.best_price) / self.best_price
        if bounce > trailing_limit:
            # Close remaining
            final_pnl_pct = (self.entry_price - current_price) / self.entry_price
            self.pnl += final_pnl_pct * self.current_size
            self.closed = True
            if self.partial_taken:
                self.close_reason = "trailing_partial_tp"
            else:
                self.close_reason = "trailing_stop"
            return True

        # 4. Timeout (24h)
        if elapsed_hours >= HOLD_HOURS:
            final_pnl_pct = (self.entry_price - current_price) / self.entry_price
            self.pnl += final_pnl_pct * self.current_size
            self.closed = True
            if self.partial_taken:
                self.close_reason = "timeout_partial_tp"
            else:
                self.close_reason = "timeout"
            return True

        return False


def run_backtest():
    print("=" * 70)
    print("  AltcoinShort â€“ èµ”ç‡/æ³¢åŠ¨ç‡/è¡Œä¸º Alpha ç»¼åˆç ”ç©¶")
    print(f"  exit optimization: Flash Crash TP & Dynamic Trailing")
    print(f"  Period: {START_DATE.date()} â†’ {END_DATE.date()}")
    print("=" * 70)

    # â”€â”€ å‘ç° tickers â”€â”€
    ticker_dirs = sorted(
        [d for d in DATA_DIR.iterdir() if d.is_dir() and d.name.endswith("usdt")]
    )
    base_tickers = {d.name.replace("usdt", "").upper(): d for d in ticker_dirs}
    eligible = {k: v for k, v in base_tickers.items() if k not in EXCLUSION}
    print(f"\n  å‘ç° {len(base_tickers)} ä¸ª tickersï¼Œæ’é™¤åå‰©ä½™ {len(eligible)} ä¸ª")

    warmup_start = START_DATE - timedelta(hours=VOL_LOOKBACK_HOURS + 24)

    # â”€â”€ åŠ è½½æ‰€æœ‰ hourly æ•°æ® â”€â”€
    all_hourly: Dict[str, pd.DataFrame] = {}
    print(f"\n  åŠ è½½æ•°æ®ä¸­ (warmup from {warmup_start.date()})...")
    loaded = 0
    for base, ddir in eligible.items():
        hourly = load_hourly_data(ddir, warmup_start, END_DATE + timedelta(days=2))
        if hourly is not None and len(hourly) >= VOL_LOOKBACK_HOURS + 48:
            all_hourly[base] = hourly
            loaded += 1
            if loaded % 20 == 0:
                print(f"    å·²åŠ è½½ {loaded} ä¸ª tickers...")
    print(f"  å…±åŠ è½½ {loaded} ä¸ª tickers")

    # â”€â”€ è®¡ç®— CSAD â”€â”€
    print("  è®¡ç®— CSAD æ¨ªæˆªé¢ç¾Šç¾¤æ•ˆåº”...")
    csad_df = compute_csad_series(all_hourly, START_DATE, END_DATE)
    herding_times = set()
    if not csad_df.empty:
        herding_rows = csad_df[csad_df["herding"]]
        herding_times = set(herding_rows["datetime"])
        herding_pct = len(herding_rows) / len(csad_df) * 100
        avg_gamma2 = csad_df["gamma2"].mean()
        print(
            f"  CSAD ç»“æœ: Î³2 å‡å€¼={avg_gamma2:.4f} | "
            f"ç¾Šç¾¤æ•ˆåº”æ—¶æ®µ: {len(herding_rows)}/{len(csad_df)} ({herding_pct:.1f}%)"
        )
    else:
        print("  CSAD: æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")

    # â”€â”€ è®¡ç®— per-symbol å› å­ â”€â”€
    print("  è®¡ç®— per-symbol å› å­...")
    co_history: deque = deque(maxlen=10000)
    all_signals: List[Dict] = []
    computed = 0

    for base, hourly in all_hourly.items():
        factors = compute_factors(hourly, co_history)
        for f in factors:
            if f["datetime"] >= START_DATE:
                f["ticker"] = base

                # åº”ç”¨ CSAD herding åŠ æˆ
                if f["datetime"] in herding_times:
                    f["weight"] = round(f["weight"] * 1.3, 4)
                    f["csad_herding"] = True
                else:
                    f["csad_herding"] = False

                all_signals.append(f)
        computed += 1
        if computed % 20 == 0:
            print(f"    å·²è®¡ç®— {computed} ä¸ª tickers...")

    print(f"  å…±ç”Ÿæˆ {len(all_signals)} æ¡åŸå§‹ä¿¡å·")

    if not all_signals:
        print("  âŒ æ²¡æœ‰ä¿¡å·")
        return

    sig_df = pd.DataFrame(all_signals)
    sig_df.sort_values(["datetime", "weight"], ascending=[True, False], inplace=True)

    signal_times = sig_df["datetime"].unique()

    # â”€â”€ ä»·æ ¼æŸ¥è¯¢è¡¨ â”€â”€
    print("  æ„å»ºä»·æ ¼æŸ¥è¯¢è¡¨...")
    price_cache: Dict[str, pd.Series] = {}
    for base, hourly in all_hourly.items():
        price_cache[base] = hourly["close"]

    # â”€â”€ æ¨¡æ‹Ÿäº¤æ˜“ â”€â”€
    print("  æ¨¡æ‹Ÿäº¤æ˜“ä¸­...")
    trades: List[Dict] = []
    active_positions: List[Position] = []
    equity_curve = []
    capital = INITIAL_CAPITAL
    position_per_slot = (INITIAL_CAPITAL * LEVERAGE) / MAX_POSITIONS

    for t in sorted(signal_times):
        for pos in active_positions:
            if pos.closed:
                continue
            if pos.ticker in price_cache:
                prices = price_cache[pos.ticker]
                mask = prices.index == t
                if mask.any():
                    pos.update(prices.loc[mask].iloc[0], t)

        newly_closed = [p for p in active_positions if p.closed]
        for p in newly_closed:
            capital += p.pnl
            trades.append(
                {
                    "ticker": p.ticker,
                    "entry_time": p.entry_time,
                    "exit_time": t,
                    "entry_price": p.entry_price,
                    "pnl": p.pnl,
                    "pnl_pct": p.pnl / p.initial_size * 100,
                    "reason": p.close_reason,
                }
            )
        active_positions = [p for p in active_positions if not p.closed]

        open_tickers = {p.ticker for p in active_positions}
        available = MAX_POSITIONS - len(active_positions)

        if available > 0:
            window = sig_df[sig_df["datetime"] == t]
            top = window[
                (~window["ticker"].isin(open_tickers)) & (window["weight"] > 0.01)
            ].head(available)

            for _, row in top.iterrows():
                active_positions.append(
                    Position(row["ticker"], row["close"], t, position_per_slot)
                )

        unrealized = 0.0
        for p in active_positions:
            if p.ticker in price_cache:
                prices = price_cache[p.ticker]
                mask = prices.index == t
                if mask.any():
                    cp = prices.loc[mask].iloc[0]
                    # Calculate unrealized PnL: realized + current floating of remaining
                    current_floating = (
                        -(cp - p.entry_price) / p.entry_price * p.current_size
                    )
                    unrealized += p.pnl + current_floating

        equity_curve.append(
            {
                "datetime": t,
                "equity": capital + unrealized,
                "positions": len(active_positions),
            }
        )

    # å¼ºåˆ¶å¹³ä»“
    for p in active_positions:
        if not p.closed and p.ticker in price_cache:
            prices = price_cache[p.ticker]
            if len(prices) > 0:
                lp = prices.iloc[-1]
                p.pnl += -(lp - p.entry_price) / p.entry_price * p.current_size
                capital += p.pnl
                trades.append(
                    {
                        "ticker": p.ticker,
                        "entry_time": p.entry_time,
                        "exit_time": prices.index[-1],
                        "entry_price": p.entry_price,
                        "pnl": p.pnl,
                        "pnl_pct": p.pnl / p.initial_size * 100,
                        "reason": "force_close",
                    }
                )

    # ================================================================
    # è¾“å‡º
    # ================================================================
    trades_df = pd.DataFrame(trades)
    equity_df = pd.DataFrame(equity_curve)

    print("\n" + "=" * 70)
    print("  ğŸ“Š å›æµ‹ç»“æœ (Optimized Exit: Flash TP + Dynamic Trailing)")
    print("=" * 70)

    if trades_df.empty:
        print("  æ²¡æœ‰äº¤æ˜“")
        return

    total_pnl = trades_df["pnl"].sum()
    win = trades_df[trades_df["pnl"] > 0]
    lose = trades_df[trades_df["pnl"] <= 0]
    win_rate = len(win) / len(trades_df) * 100

    avg_win = win["pnl_pct"].mean() if len(win) > 0 else 0
    avg_loss = lose["pnl_pct"].mean() if len(lose) > 0 else 0
    pf = (
        abs(win["pnl"].sum() / lose["pnl"].sum())
        if len(lose) > 0 and lose["pnl"].sum() != 0
        else float("inf")
    )

    print(f"  æ€»äº¤æ˜“æ•°:     {len(trades_df)}")
    print(f"  èƒœç‡:         {win_rate:.1f}%")
    print(f"  ç›ˆåˆ©äº¤æ˜“:     {len(win)}  (å¹³å‡ {avg_win:+.2f}%)")
    print(f"  äºæŸäº¤æ˜“:     {len(lose)}  (å¹³å‡ {avg_loss:+.2f}%)")
    print(f"  ç›ˆäºæ¯” (PF):  {pf:.2f}")
    print(f"  æ€» PnL:       ${total_pnl:,.2f}")
    print(f"  æ€»æ”¶ç›Šç‡:     {total_pnl / INITIAL_CAPITAL * 100:+.2f}%")

    if not equity_df.empty:
        peak = equity_df["equity"].expanding().max()
        dd = (equity_df["equity"] - peak) / peak
        max_dd = dd.min() * 100
        print(f"  æœ€å¤§å›æ’¤:     {max_dd:.2f}%")

    # â”€â”€ å¹³ä»“åŸå›  â”€â”€
    print(f"\n  å¹³ä»“åŸå› åˆ†å¸ƒ:")
    for reason, count in trades_df["reason"].value_counts().items():
        sub = trades_df[trades_df["reason"] == reason]
        print(f"    {reason:20s}: {count:4d} ç¬”  (avg {sub['pnl_pct'].mean():+.2f}%)")

    # â”€â”€ Top/Bottom tickers â”€â”€
    if len(trades_df) > 0:
        tp = trades_df.groupby("ticker")["pnl"].sum().sort_values()
        print(f"\n  ğŸ“‰ äºæŸæœ€å¤š Top 5 tickers:")
        for t, pnl in tp.head(5).items():
            print(f"    {t:10s}: ${pnl:+,.2f}")
        print(f"\n  ğŸ“ˆ ç›ˆåˆ©æœ€å¤š Top 5 tickers:")
        for t, pnl in tp.tail(5).items():
            print(f"    {t:10s}: ${pnl:+,.2f}")

    # â”€â”€ ä¿å­˜ â”€â”€
    out_dir = Path(__file__).parent / "backtests"
    out_dir.mkdir(exist_ok=True)

    tp = out_dir / "alpha_exit_trades.csv"
    ep = out_dir / "alpha_exit_equity.csv"
    sp = out_dir / "alpha_exit_signals.csv"

    trades_df.to_csv(tp, index=False)
    equity_df.to_csv(ep, index=False)
    sig_df.to_csv(sp, index=False)

    print(f"\n  ç»“æœå·²ä¿å­˜:")
    print(f"    äº¤æ˜“è®°å½•:  {tp}")
    print(f"    æƒç›Šæ›²çº¿:  {ep}")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    run_backtest()
